集成学习就是要发挥集体决策的优势，多数投票法以单个分类模型的分类结果为基础，采用少数服从多数的原则确定模型预测的类别标签。

集成方法就是将不同的分类器合并为一个元分类器，使其具有比单个分类器更好地泛适应能力，下面就介绍集成方法中最为典型的方法：绝对多数投票法。

绝对多数投票法意味着，我们将选择已经被大多数分类器预测过的类标签，也就是说，总体上超过了50%的票数。

相对多数投票法与绝对多数投票法相对应，它以票数最多为标准来确定类标签。

为了在概念上区分，用圆形、三角形、正方形代表类标签，如下图所示：

 ![842de1a3-b870-4195-bc96-50b268abf37f](/Users/rachael-y/Downloads/842de1a3-b870-4195-bc96-50b268abf37f.png)



从上图可以看出，第二行中蓝色圆形有6个，超过50%，占据绝对多数，而第三行中圆形相对于三角形和正方形，占比40%最多，属于相对多数。

 

集成学习可以由不同的分类算法构建，例如决策树、支持向量机、逻辑回归等。



也可以说，我们能够使用相同的分类算法，拟合训练数据集的不同子集，最典型的例子就是随机森林算法，它合并了多个不同的决策树分类器。

 

采用绝对多数投票法的一般集成方法实现原理，如下图所示：

 

![53d6fefc-6081-472b-a4d5-34c0f43350e9](/Users/rachael-y/Downloads/53d6fefc-6081-472b-a4d5-34c0f43350e9.png)

 

从上图可以看出，不同的分类模型基于训练数据分别预测，然后对全部预测结果（类标签）投票，预测结果中占多数的类标签胜出。



这种方法也体现了集体决策能力大于个体决策能力的思维：单个分类器的预测结果放入到投票箱中后，根据得票多寡决定胜出的类别标签。

 

下面我们看一下基本分类与集成分类的错误对比，以便更好地认识集成分类的优势，如下图所示：



![640-2](/Users/rachael-y/Desktop/640-2.webp)

从上图中可以看出，只要基本分类器的性能优于随机猜测，集成的错误概率总是优于单个基本分类器的错误概率。

 

下面的实现的算法能够让我们结合不同的分类算法和单独的权重以获得置信度，目标是构建一个更强大的元分类器，以平衡单个分类器在特定数据集上的弱点。



ROC AUC面积计算结果如下图所示：

![640-3](/Users/rachael-y/Desktop/640-3.webp)



从上图可以看出，多数投票分类器超过全部单个分类器的性能。下面以鸢尾花分类为例，对比单个分类器分类能力和多数投票法的差别，如下图所示：

![图片](https://mmbiz.qpic.cn/sz_mmbiz_png/0sjkh9Qia8gicyhkj4oLelphsJJkXiaYTqnMyCOA3zduHa18XvO8AhK0zFfD7EETLtbGtS32whXriaiadXvEyGwvxWw/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)



从上图可以看出，多数投票法相比于其他3个分类算法分类效果要好一些。

 

**内容小结**



**集成学习的原理是发挥群体决策的智慧，多数投票法以单个分类模型的分类结果为基础，采用少数服从多数的原则确定模型预测的类别标签。**



**名词术语**





Majority voting：绝对多数投票法



Plurality voting：相对多数投票法



Weighed voting：加权投票法



Binary class：二分类



Multiclass：多级分类



![640-4](/Users/rachael-y/Desktop/640-4.webp)
