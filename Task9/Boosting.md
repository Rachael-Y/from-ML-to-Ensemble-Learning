
关于 Boosting
 

在「顺序化的方法中」，组合起来的不同弱模型之间不再相互独立地拟合。其思想是「迭代地」拟合模型，使模型在给定步骤上的训练依赖于之前的步骤上拟合的模型。「Boosting」是这些方法中最著名的一种，它生成的集成模型通常比组成该模型的弱学习器偏置更小。

    

提升方法
 

Boosting 方法和bagging 方法的工作思路是一样的：我们构建一系列模型，将它们聚合起来得到一个性能更好的强学习器。然而，与重点在于减小方差的 bagging 不同，boosting 着眼于以一种适应性很强的方式顺序拟合多个弱学习器：序列中每个模型在拟合的过程中，会更加重视那些序列中之前的模型处理地很糟糕的观测数据。

 

直观地说，每个模型都把注意力集中在目前最难拟合的观测数据上。这样一来，在这个过程的最后，我们就获得了一个具有较低偏置的强学习器（我们会注意到，boosting 也有减小方差的效果）。和 bagging 一样，Boosting 也可以用于回归和分类问题。

 

由于其重点在于减小偏置，用于 boosting  的基础模型通常是那些低方差高偏置的模型。例如，如果想要使用树作为基础模型，我们将主要选择只有少许几层的较浅决策树。

 

而选择低方差高偏置模型作为 boosting 弱学习器的另一个重要原因是：这些模型拟合的计算开销较低（参数化时自由度较低）。

 

实际上，由于拟合不同模型的计算无法并行处理（与 bagging 不同），顺序地拟合若干复杂模型会导致计算开销变得非常高。

   

一旦选定了弱学习器，我们仍需要定义它们的拟合方式（在拟合当前模型时，要考虑之前模型的哪些信息？）和聚合方式（如何将当前的模型聚合到之前的模型中？）在接下来的两小节中，我们将讨论这些问题，尤其是介绍两个重要的 boosting 算法：自适应提升（adaboost ）和梯度提升（gradient boosting）。

 

简而言之，这两种元算法在顺序化的过程中创建和聚合弱学习器的方式存在差异。自适应增强算法会更新附加给每个训练数据集中观测数据的权重，而梯度提升算法则会更新这些观测数据的值。这里产生差异的主要原因是：两种算法解决优化问题（寻找最佳模型——弱学习器的加权和）的方式不同。 


![af2eb2f7df03f9cb7b8a2c15d4ab34e](https://user-images.githubusercontent.com/62379948/115120978-f6faf800-9fe2-11eb-90f4-d53d59ebcc8c.png)


Boosting 会迭代地拟合一个弱学习器，将其聚合到集成模型中，并「更新」训练数据集，从而在拟合下一个基础模型时更好地考虑当前集成模型的优缺点。

 

自适应 boosting
 

在自适应 boosting（通常被称为「adaboost」）中，我们将集成模型定义为 L 个弱学习器的加权和图片，其中 c_l 是系数而 w_l 是弱学习器

  

寻找这种最佳集成模型是一个「困难的优化问题」。因此，我们并没打算一次性地解决该问题（找到给出最佳整体加法模型的所有系数和弱学习器），而是使用了一种更易于处理的「迭代优化过程」（即使它有可能导致我们得到次优解）。

 

另外，我们将弱学习器逐个添加到当前的集成模型中，在每次迭代中寻找可能的最佳组合（系数、弱学习器）。换句话说，我们循环地将 s_l 定义如下： 


![b52036f4600e780c754836e6f64eaa5](https://user-images.githubusercontent.com/62379948/115120991-07ab6e00-9fe3-11eb-90e1-1dc2db60f7f4.png)


其中，c_l 和 w_l 被挑选出来，使得 s_l 是最适合训练数据的模型，因此这是对 s_(l-1) 的最佳可能改进。我们可以进一步将其表示为：

![96a0359817c6402ef2d8a093421f973](https://user-images.githubusercontent.com/62379948/115121006-172ab700-9fe3-11eb-8543-16a289dc0ab4.png)


其中，E(.) 是给定模型的拟合误差，e(.,.)是损失/误差函数。因此，我们并没有在求和过程中对所有 L 个模型进行「全局优化」，而是通过「局部」优化来近似最优解并将弱学习器逐个添加到强模型中。

 

更特别的是，在考虑二分类问题时，我们可以将 adaboost 算法重新写入以下过程：首先，它将更新数据集中观测数据的权重，训练一个新的弱学习器，该学习器重点关注当前集成模型误分类的观测数据。其次，它会根据一个表示该弱模型性能的更新系数，将弱学习器添加到加权和中：弱学习器的性能越好，它对强学习器的贡献就越大。

 

因此，假设我们面对的是一个二分类问题：数据集中有 N 个观测数据，我们想在给定一组弱模型的情况下使用 adaboost 算法。在算法的起始阶段（序列中的第一个模型），所有的观测数据都拥有相同的权重「1/N」。然后，我们将下面的步骤重复 L 次（作用于序列中的 L 个学习器）：

  

用当前观测数据的权重拟合可能的最佳弱模型

计算更新系数的值，更新系数是弱学习器的某种标量化评估指标，它表示相对集成模型来说，该弱学习器的分量如何

通过添加新的弱学习器与其更新系数的乘积来更新强学习器

计算新观测数据的权重，该权重表示我们想在下一轮迭代中关注哪些观测数据（聚和模型预测错误的观测数据的权重增加，而正确预测的观测数据的权重减小）

  

重复这些步骤，我们顺序地构建出 L 个模型，并将它们聚合成一个简单的线性组合，然后由表示每个学习器性能的系数加权。注意，初始 adaboost 算法有一些变体，比如 LogitBoost（分类）或 L2Boost（回归），它们的差异主要取决于损失函数的选择。

 

![f9b8c3cfa6e7217d191dd2d75063626](https://user-images.githubusercontent.com/62379948/115120969-e9457280-9fe2-11eb-8772-a6514f6baa05.png)


Adaboost 在每次迭代中更新观测数据的权重。正确分类的观测数据的权重相对于错误分类的观测数据的权重有所下降。在最终的集成模型中，性能更好的模型具有更高的权重。
